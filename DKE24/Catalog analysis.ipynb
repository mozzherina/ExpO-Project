{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "innovative-nigeria",
   "metadata": {},
   "source": [
    "# Experiments with OntoUML Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_implement = os.getcwd()\n",
    "os.chdir(\"Images\")\n",
    "dir_images = os.getcwd()\n",
    "os.chdir(\"../Abstractions\")\n",
    "dir_abstractions = os.getcwd()\n",
    "os.chdir(\"../Errors\")\n",
    "dir_errors = os.getcwd()\n",
    "os.chdir(\"../../../GitHub/ontouml-models2/models\")\n",
    "dir_models = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "sns.despine(offset=5, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors={\n",
    "    'super small': 'magenta', \n",
    "    'small': 'green', \n",
    "    'medium': 'blue', \n",
    "    'big': 'orange', \n",
    "    'super big': 'indigo'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-morgan",
   "metadata": {},
   "source": [
    "----\n",
    "## Preparing subsets of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-humanitarian",
   "metadata": {},
   "source": [
    "### List of potential models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-vehicle",
   "metadata": {},
   "source": [
    "Setting a directory with models as a working directory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir_models)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_problems = [\n",
    "    'digitaldoctor2022/ontology.json',\n",
    "    'goncalves2011ecg/ontology.json',\n",
    "    'tourbo2021/ontology.json',\n",
    "    'plato-ontology2019/ontology.json',\n",
    "    'buridan-ontology2021/ontology.json',\n",
    "    'aristotle-ontology2019/ontology.json',\n",
    "    'public-expense-ontology2020/ontology.json',\n",
    "    'tender2013/ontology.json',\n",
    "    'scientific-publication2013/ontology.json'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-amendment",
   "metadata": {},
   "source": [
    "Go to the folder with models and scan it for *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_names = []\n",
    "for file in glob.glob(\"*/ontology.json\"):\n",
    "    if file not in json_problems:\n",
    "        all_file_names.append(file)\n",
    "\n",
    "print(f\"We have {len(all_file_names)} files with ontologies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-garbage",
   "metadata": {},
   "source": [
    "In order to select only those models, that contains only 16 stereotypes (those, for which the algorithm was developed), we\n",
    "1. analyse all models\n",
    "2. filter those of our interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(stereotype: str) -> str:\n",
    "    if stereotype:\n",
    "        stereotype = stereotype.lower().replace(\" \", \"\")\n",
    "    return stereotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_stereotypes(contents, all_content) -> dict:\n",
    "    if contents:\n",
    "        for content in contents:\n",
    "            if content['type'] == 'Package':\n",
    "                all_content = get_all_stereotypes(content['contents'], all_content)\n",
    "            else:\n",
    "                if content['type'] == 'Class':\n",
    "                    if 'stereotype' in content.keys():\n",
    "                        stereotype = content['stereotype']\n",
    "                        stereotype = normalize(stereotype)\n",
    "                        #if stereotype:\n",
    "                        if stereotype in all_content:\n",
    "                            all_content[stereotype] += 1\n",
    "                        else:\n",
    "                            all_content[stereotype] = 1\n",
    "                    elif 'stereotypes' in content.keys():\n",
    "                        if content['stereotypes']:\n",
    "                            for stereotype in content['stereotypes']:\n",
    "                                if stereotype in all_content:\n",
    "                                    all_content[stereotype] += 1\n",
    "                                else:\n",
    "                                    all_content[stereotype] = 1\n",
    "    return all_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stereotypes = pd.DataFrame(columns=['Name'])\n",
    "\n",
    "for file_name in all_file_names:\n",
    "    file = open(file_name, encoding=\"ISO-8859-1\", mode=\"r\")\n",
    "    data = json.loads(file.read())\n",
    "    if 'model' in data.keys():\n",
    "        contents = data['model']['contents']\n",
    "        model_stereotypes = get_all_stereotypes(contents, {})\n",
    "        model_stereotypes['Name'] = file_name.split('/')[0]\n",
    "        df_stereotypes = df_stereotypes.append(model_stereotypes, ignore_index = True)        \n",
    "    else:\n",
    "        print(f\"ERROR: Model not found in {file_name}.\")\n",
    "    file.close()\n",
    "\n",
    "df_stereotypes = df_stereotypes.fillna(0)\n",
    "df_stereotypes = df_stereotypes.set_index('Name')\n",
    "df_stereotypes = df_stereotypes.astype(int)\n",
    "\n",
    "print(f\"We have stereotypes for {len(df_stereotypes)} ontologies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stereotypes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-alias",
   "metadata": {},
   "source": [
    "Just for curiosity, what are the most popular stereotypes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stereotypes.sum().sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-stopping",
   "metadata": {},
   "source": [
    "Filtering only those models, that can be processed by the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_stereotypes = [\n",
    "    'subkind', 'kind', 'role', 'relator', 'category', \n",
    "    'event', 'rolemixin', 'mode', 'phase', 'collective',  \n",
    "    'datatype', 'quality', 'mixin', 'quantity', \n",
    "    'enumeration', 'phasemixin'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df_stereotypes.columns.difference(algorithm_stereotypes)\n",
    "not_supported_models = df_stereotypes[df_stereotypes[subset].sum(axis=1) > 0].index\n",
    "print(f\"Number of models that contains not supported class stereotypes: {len(not_supported_models)}\")\n",
    "\n",
    "df_models = df_stereotypes.loc[~df_stereotypes.index.isin(not_supported_models), \n",
    "                               ~df_stereotypes.columns.isin(subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_file_names = [name for name in all_file_names if name.split('/')[0] in df_models.index]\n",
    "print(f\"Number of models that can be processed: {len(potential_file_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(contents, all_content) -> dict:\n",
    "    if contents:\n",
    "        for content in contents:\n",
    "            if content['type'] == 'Package':\n",
    "                all_content = get_content(content['contents'], all_content)\n",
    "            else:\n",
    "                if content['type'] == 'Class':\n",
    "                    all_content['Classes'] += 1\n",
    "                elif content['type'] == 'Relation':\n",
    "                    if (content['properties'][0]['aggregationKind'] == 'COMPOSITE') | (\n",
    "                        content['properties'][1]['aggregationKind'] == 'COMPOSITE'):\n",
    "                        all_content['PartOf'] += 1\n",
    "                    all_content['Relations'] += 1\n",
    "                elif content['type'] == 'Generalization':\n",
    "                    all_content['Generalizations'] += 1\n",
    "                    all_content['Relations'] += 1\n",
    "    return all_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_potential = pd.DataFrame(columns=['Name', 'Classes', 'Relations', 'Generalizations', 'PartOf'])\n",
    "\n",
    "for file_name in potential_file_names:\n",
    "    file = open(file_name, encoding=\"ISO-8859-1\", mode=\"r\")\n",
    "    data = json.loads(file.read())\n",
    "    contents = None\n",
    "    if 'contents' in data.keys():\n",
    "        contents = data['contents']\n",
    "    elif 'model' in data.keys():\n",
    "        contents = data['model']['contents']\n",
    "    else:\n",
    "        print(f\"ERROR: Neither model nor contents found in {file_name}.\")\n",
    "    file.close()\n",
    "    \n",
    "    all_content = get_content(contents, \n",
    "                              {\n",
    "                                  'Classes': 0, \n",
    "                                  'Relations': 0,\n",
    "                                  'PartOf': 0,\n",
    "                                  'Generalizations': 0\n",
    "                              })\n",
    "    all_content['Name'] = file_name.split('/')[0]\n",
    "    df_potential = df_potential.append(all_content, ignore_index = True)\n",
    "    \n",
    "df_potential = df_potential.fillna(0)\n",
    "df_potential = df_potential.set_index('Name')\n",
    "df_potential = df_potential.astype(int)\n",
    "\n",
    "print(f\"We have statistics for {len(df_potential)} models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_potential.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_potential['TotalSize'] = df_potential['Classes'] + df_potential['Relations']\n",
    "print(df_potential['TotalSize'].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (df_potential['TotalSize'] >= 1000),\n",
    "    (df_potential['TotalSize'] < 1000) & (df_potential['TotalSize'] >= 200),\n",
    "    (df_potential['TotalSize'] < 200) & (df_potential['TotalSize'] >= 75),\n",
    "    (df_potential['TotalSize'] < 75) & (df_potential['TotalSize'] >= 35),\n",
    "    (df_potential['TotalSize'] < 35)\n",
    "]\n",
    "values = ['super big', 'big', 'medium', 'small', 'super small']\n",
    "df_potential['Model size'] = np.select(conditions, values)\n",
    "df_potential.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir_images)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(7)\n",
    "\n",
    "for (t,c) in colors.items():\n",
    "    sel_df = df_potential[df_potential['Model size']==t]\n",
    "    scatter = ax.scatter(sel_df['Classes'], sel_df['Relations'], s=sel_df['TotalSize'], \n",
    "                         alpha=0.5, c=c, cmap='viridis', label=t)\n",
    "\n",
    "#plt.title(\"Size of conceptual models\", fontsize=16)\n",
    "plt.xlabel(\"Number of classes\", fontsize=16)\n",
    "plt.ylabel(\"Number of relations\", fontsize=16)\n",
    "\n",
    "lgnd = plt.legend(markerscale=1,scatterpoints=1, fontsize=14)\n",
    "\n",
    "#change the marker size manually for all\n",
    "lgnd.legendHandles[0]._sizes = [50]\n",
    "lgnd.legendHandles[1]._sizes = [50]\n",
    "lgnd.legendHandles[2]._sizes = [50]\n",
    "lgnd.legendHandles[3]._sizes = [50]\n",
    "lgnd.legendHandles[4]._sizes = [50]\n",
    "#plt.show()\n",
    "plt.savefig('all_models.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-bristol",
   "metadata": {},
   "source": [
    "### List of valid models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-justice",
   "metadata": {},
   "source": [
    "Send request to `api.ontouml.org` and check models for validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Accept': \"application/json\",\n",
    "    'Connection': \"keep-alive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_verify = \"http://api.ontouml.org/v1/verify\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-prisoner",
   "metadata": {},
   "source": [
    "List of fixed models:\n",
    "1. bernasconi2023fair-principles ontology 1 errors were found\n",
    "2. In goncalves2011ecg ontology 1 errors were found\n",
    "3. In gomes2022digital-technology ontology 1 errors were found\n",
    "4. In eu-rent-refactored2022 ontology 2 errors were found\n",
    "5. In health-organizations ontology 5 errors were found\n",
    "6. In srro-ontology ontology 2 errors were found\n",
    "7. In aguiar2019ooco ontology 3 errors were found\n",
    "8. In nardi2015ufo-s ontology 1 errors were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir_models)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_file_names = []\n",
    "\n",
    "for file_name in potential_file_names:\n",
    "    file = open(file_name, encoding=\"ISO-8859-1\", mode=\"r\")\n",
    "    data = json.loads(file.read())\n",
    "    file.close()\n",
    "\n",
    "    body = {'project': data}\n",
    "    response = requests.post(url_verify, headers=headers, json=body)\n",
    "    responseResults = json.loads(response.text)['result']\n",
    "    if len(responseResults) == 0:\n",
    "        valid_file_names.append(file_name)\n",
    "    else:\n",
    "        print(f\"In {file_name.split('/')[0]} ontology {len(responseResults)} errors were found\")\n",
    "\n",
    "print(f\"Number of valid ontologies is {len(valid_file_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of valid ontologies is {len(valid_file_names)}\")   \n",
    "print(f\"Number of potential ontologies is {len(potential_file_names)}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df_potential.loc[df_potential.index.isin([name.split('/')[0] for name in valid_file_names]), :]\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir_images)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(7)\n",
    "\n",
    "library = df_valid.loc['romanenko2023what']\n",
    "for (t,c) in colors.items():\n",
    "    sel_df = df_valid[df_valid['Model size']==t]\n",
    "    scatter = ax.scatter(sel_df['Classes'], sel_df['Relations'], s=sel_df['TotalSize'], \n",
    "                         alpha=0.5, c=c, cmap='viridis', label=t)\n",
    "\n",
    "ax.scatter(library['Classes'], library['Relations'], s=library['TotalSize'], \n",
    "            alpha=1.0, c='red', marker='s', cmap='viridis', label='library model')\n",
    "\n",
    "#plt.title(\"Size of conceptual models\", fontsize=16)\n",
    "plt.xlabel(\"Number of classes\", fontsize=16)\n",
    "plt.ylabel(\"Number of relations\", fontsize=16)\n",
    "\n",
    "lgnd = plt.legend(markerscale=1,scatterpoints=1, fontsize=14)\n",
    "\n",
    "#change the marker size manually for all\n",
    "lgnd.legendHandles[0]._sizes = [50]\n",
    "lgnd.legendHandles[1]._sizes = [50]\n",
    "lgnd.legendHandles[2]._sizes = [50]\n",
    "lgnd.legendHandles[3]._sizes = [50]\n",
    "lgnd.legendHandles[4]._sizes = [50]\n",
    "lgnd.legendHandles[5]._sizes = [50]\n",
    "\n",
    "plt.savefig('valid_models.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-olympus",
   "metadata": {},
   "source": [
    "----\n",
    "## Running abstractions on different sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-shakespeare",
   "metadata": {},
   "source": [
    "### Checking valid models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir_models)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_abstract = \"https://expose.eng.unibz.it/abstract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "atypes = {\n",
    "    \"h\": ['hierarchy'], \n",
    "    \"a\": ['aspects'], \n",
    "    \"p\": ['parthood'],\n",
    "    \"ha\": ['hierarchy', 'aspects'],\n",
    "    \"ap\": ['parthood', 'aspects'],\n",
    "    \"hp\": ['parthood', 'hierarchy'],\n",
    "    \"full\": ['parthood', 'hierarchy', 'aspects']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for file_name in valid_file_names:\n",
    "    model_name = file_name.split(os.path.sep)[0]\n",
    "    file = open(file_name, encoding=\"ISO-8859-1\", mode=\"r\")\n",
    "    data = json.loads(file.read())\n",
    "    \n",
    "    for abstr_name, abstr_params in atypes.items():\n",
    "        response = requests.post(url_abstract, headers=headers,\n",
    "                                 json={\n",
    "                                     'abs_type': abstr_params,\n",
    "                                     'long_names': True,\n",
    "                                     'mult_relations': False,\n",
    "                                     'keep_relators': True,\n",
    "                                     'in_format': 'json',\n",
    "                                     'out_format': 'json',\n",
    "                                     'height': 1000,\n",
    "                                     'width': 1000,\n",
    "                                     'origin': data\n",
    "                                     #'origin': json.load(open(file_name))\n",
    "                                 })\n",
    "\n",
    "        if response.ok:\n",
    "            new_file_name = f\"{dir_abstractions}{os.path.sep}{model_name}_{abstr_name}.json\"\n",
    "            with open(new_file_name, 'w') as f:\n",
    "                json.dump(response.json(), f)\n",
    "\n",
    "print(f\"All valid models were processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir_abstractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstraction_file_names = []\n",
    "for file in glob.glob(\"*.json\"):\n",
    "    abstraction_file_names.append(file)\n",
    "\n",
    "print(f\"We have {len(abstraction_file_names)} files with abstractions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-relaxation",
   "metadata": {},
   "source": [
    "__Validation check of abstracted models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_abstract = pd.DataFrame(columns=['Name', 'Classes', 'Relations', 'Generalizations', 'PartOf'])\n",
    "\n",
    "for file_name in abstraction_file_names:\n",
    "    file = open(file_name, encoding=\"ISO-8859-1\", mode=\"r\")\n",
    "    data = json.loads(file.read())\n",
    "    contents = None\n",
    "    if 'contents' in data.keys():\n",
    "        contents = data['contents']\n",
    "    elif 'model' in data.keys():\n",
    "        contents = data['model']['contents']\n",
    "    else:\n",
    "        print(f\"ERROR: Neither model nor contents found in {file_name}.\")\n",
    "    file.close()\n",
    "    \n",
    "    all_content = get_content(contents, \n",
    "                              {\n",
    "                                  'Classes': 0, \n",
    "                                  'Relations': 0,\n",
    "                                  'PartOf': 0,\n",
    "                                  'Generalizations': 0\n",
    "                              })\n",
    "    all_content['Name'] = file_name.split('/')[0][:-5]\n",
    "    \n",
    "    response = requests.post(url_verify, headers=headers, json={'project': data})\n",
    "    responseResults = json.loads(response.text)\n",
    "    if 'result' not in responseResults:\n",
    "        print(all_content['Name'] + \": \" + responseResults['message'])\n",
    "    df_abstract = df_abstract.append(all_content, ignore_index = True)\n",
    "    \n",
    "df_abstract = df_abstract.fillna(0)\n",
    "\n",
    "print(f\"We have statistics for {len(df_abstract)} abstractions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "anames = {\n",
    "    \"h\": 'hierarchy', \n",
    "    \"a\": 'aspects', \n",
    "    \"p\": 'parthood',\n",
    "    \"ha\": 'aspects and hierarchy',\n",
    "    \"ap\": 'parthood and aspects',\n",
    "    \"hp\": 'parthood and hierarchy',\n",
    "    \"full\": 'full abstraction'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract[\"TotalSize\"] = df_abstract[\"Classes\"] + df_abstract[\"Relations\"]\n",
    "df_abstract['Model size'] = \"\"\n",
    "df_abstract[\"Type of abstraction\"] = df_abstract[\"Name\"].str.rsplit('_', 1).str[1].map(anames)\n",
    "df_abstract[\"Name\"] = df_abstract[\"Name\"].str.rsplit('_', 1).str[0]\n",
    "size_dict = pd.Series(df_valid['Model size'].values,index=df_valid.index).to_dict()\n",
    "df_abstract['Model size'] = df_abstract['Name'].map(size_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_models = df_valid.copy(deep=True).reset_index()\n",
    "original_models[\"Type of abstraction\"] = 'original model'\n",
    "original_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract = pd.concat([df_abstract,original_models], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract[df_abstract['Name']=='romanenko2023what']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_types = ['original model', 'aspects', 'parthood', \n",
    "             'parthood and aspects', 'hierarchy', \n",
    "             'aspects and hierarchy', 'parthood and hierarchy',\n",
    "             'full abstraction']\n",
    "abstraction_type = CategoricalDtype(abs_types, ordered=True)\n",
    "df_abstract['Type of abstraction'] = df_abstract['Type of abstraction'].astype(abstraction_type)\n",
    "df_abstract.sort_values(by='Type of abstraction', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir_images)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract.to_excel(\"output.xlsx\", sheet_name='abstractions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_labels(ax, width, break_long_words=False):\n",
    "    labels = []\n",
    "    for label in ax.get_xticklabels():\n",
    "        text = label.get_text()\n",
    "        labels.append(textwrap.fill(text, width=width,\n",
    "                      break_long_words=break_long_words))\n",
    "    ax.set_xticklabels(labels, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "sns.lineplot(x=\"Type of abstraction\", y=\"Classes\",\n",
    "            hue=\"Model size\", palette=colors.values(),\n",
    "            data=df_abstract, ax=ax, sort=False)\n",
    "ax.set_xticklabels(abs_types)\n",
    "wrap_labels(ax, 18)\n",
    "sns.despine(offset=10, trim=True)\n",
    "plt.xlabel('Type of abstraction', fontsize=16)\n",
    "plt.ylabel('Number of classes', fontsize=16)\n",
    "#plt.show()\n",
    "plt.savefig('classes_compression.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "sns.lineplot(x=\"Type of abstraction\", y=\"Relations\",\n",
    "            hue=\"Model size\", palette=colors.values(),\n",
    "            data=df_abstract, ax=ax, sort=False)\n",
    "ax.set_xticklabels(abs_types)\n",
    "wrap_labels(ax, 18)\n",
    "sns.despine(offset=10, trim=True)\n",
    "plt.xlabel('Type of abstraction', fontsize=16);\n",
    "plt.ylabel('Number of relations', fontsize=16);\n",
    "#plt.show()\n",
    "plt.savefig('relations_compression.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-washer",
   "metadata": {},
   "source": [
    "### Checking potential models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir_models)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_file_names = list(set(potential_file_names) - set(valid_file_names))\n",
    "len(error_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for file_name in error_file_names:\n",
    "    model_name = file_name.split(os.path.sep)[0]\n",
    "    \n",
    "    for abstr_name, abstr_params in atypes.items():\n",
    "        file = open(file_name, encoding=\"ISO-8859-1\", mode=\"r\")\n",
    "        data = json.loads(file.read())\n",
    "        response = requests.post(url_abstract, headers=headers,\n",
    "                                 json={\n",
    "                                     'abs_type': abstr_params,\n",
    "                                     'long_names': True,\n",
    "                                     'mult_relations': False,\n",
    "                                     'keep_relators': True,\n",
    "                                     'in_format': 'json',\n",
    "                                     'out_format': 'json',\n",
    "                                     'height': 1000,\n",
    "                                     'width': 1000,\n",
    "                                     'origin': data\n",
    "                                 })\n",
    "\n",
    "        if response.ok:\n",
    "            new_file_name = f\"{dir_errors}{os.path.sep}{model_name}_{abstr_name}.json\"\n",
    "            with open(new_file_name, 'w') as f:\n",
    "                json.dump(response.json(), f)\n",
    "\n",
    "print(f\"All models with errors were processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error = pd.DataFrame(columns=['Name'] + abs_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in error_file_names:\n",
    "    file = open(file_name, encoding=\"ISO-8859-1\", mode=\"r\")\n",
    "    data = json.loads(file.read())\n",
    "    file.close()\n",
    "\n",
    "    body = {'project': data}\n",
    "    response = requests.post(url_verify, headers=headers, json=body)\n",
    "    responseResults = json.loads(response.text)['result']\n",
    "    df_error = df_error.append({'Name':file_name.split('/')[0], \n",
    "                                'original model': len(responseResults)}, \n",
    "                               ignore_index = True)\n",
    "df_error = df_error.set_index('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstraction_error_file_names = []\n",
    "for file in glob.glob(\"*.json\"):\n",
    "    abstraction_error_file_names.append(file)\n",
    "\n",
    "print(f\"We have {len(abstraction_error_file_names)} files with abstractions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for file_name in abstraction_error_file_names:\n",
    "    file = open(file_name, encoding=\"ISO-8859-1\", mode=\"r\")\n",
    "    data = json.loads(file.read())\n",
    "    response = requests.post(url_verify, headers=headers, json={'project': data})\n",
    "    responseResults = json.loads(response.text)\n",
    "    if 'result' not in responseResults:\n",
    "        print(file_name + \": \" + responseResults['message'])\n",
    "    else:\n",
    "        name, abstraction = file_name.rsplit('_', 1)\n",
    "        df_error.loc[name, anames[abstraction[:-5]]] = len(responseResults['result'])\n",
    "df_error = df_error.fillna(0)\n",
    "df_error = df_error.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-jacksonville",
   "metadata": {},
   "source": [
    "### Complete check of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir_models)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_models = 0\n",
    "for idx, file_name in enumerate(valid_file_names):\n",
    "    model_name = file_name.split(os.path.sep)[0]\n",
    "    print(f\"({idx}) {model_name}\")\n",
    "    \n",
    "    file = open(file_name, encoding=\"ISO-8859-1\", mode=\"r\")\n",
    "    data = json.loads(file.read())\n",
    "    rule = \"start\"\n",
    "    applied_rules = []\n",
    "    \n",
    "    while rule:\n",
    "        response = requests.post(url_abstract, headers=headers,\n",
    "                                 json={\n",
    "                                     'abs_type': [],\n",
    "                                     'long_names': True,\n",
    "                                     'mult_relations': False,\n",
    "                                     'keep_relators': True,\n",
    "                                     'in_format': 'json',\n",
    "                                     'out_format': 'expo',\n",
    "                                     'height': 1000,\n",
    "                                     'width': 1000,\n",
    "                                     'origin': data\n",
    "                                 })\n",
    "        if response.ok:\n",
    "            all_models += 1\n",
    "            abstraction = json.loads(response.text)\n",
    "            rule = abstraction[\"rule\"]\n",
    "            applied_rules.append(rule)\n",
    "            data = abstraction[\"origin\"]\n",
    "            \n",
    "            response = requests.post(url_verify, headers=headers, json={'project': data})\n",
    "            responseResults = json.loads(response.text)\n",
    "            if 'result' not in responseResults:\n",
    "                print(f\"ERROR: Not valid abstraction of {model_name} at step {len(applied_rules)}.\")\n",
    "                print(responseResults['message'])    \n",
    "                # rule = \"\"\n",
    "                # break\n",
    "            \n",
    "        else:\n",
    "            print(f\"ERROR: Cannot abstract model {model_name} at step {len(applied_rules)}.\")\n",
    "            rule = \"\"\n",
    "            break\n",
    "    \n",
    "    print(\", \".join(applied_rules)[:-2])  \n",
    "print(f\"Total number of all models is {all_models}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
